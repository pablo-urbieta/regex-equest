{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxf9w6pMqaLWuHmGIvORMF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pablo-urbieta/regex-equest/blob/main/Regex_GenAI_GIT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSFdGNBqJP80",
        "outputId": "92203184-2d8c-4601-f3bd-0898d7dad081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.0\n",
            "Collecting openai==1.40.0\n",
            "  Downloading openai-1.40.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.40.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.40.0) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai==1.40.0)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai==1.40.0)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.40.0) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.40.0) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.40.0) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai==1.40.0) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.40.0) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.40.0) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.40.0) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai==1.40.0)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.40.0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.40.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.40.0) (2.20.1)\n",
            "Downloading openai-1.40.0-py3-none-any.whl (360 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m360.4/360.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-dotenv, jiter, h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 openai-1.40.0 python-dotenv-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit -q\n",
        "!pip install xlsxwriter\n",
        "!pip install openai==1.40.0 python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import os\n",
        "import streamlit as st\n",
        "import re\n",
        "\n",
        "# Carregar variáveis de ambiente\n",
        "os.environ['OPENAI_API_KEY'] = st.secrets[\"openai\"][\"api_key\"]\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "\n",
        "#Stremlit\n",
        "import io\n",
        "from io import StringIO\n",
        "import xlsxwriter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def extraction():\n",
        "  global content_filtered\n",
        "  output_file_path = '/content/extracted_sections.txt'\n",
        "\n",
        "  # Step 2: Find all sections labeled \"REPORT- SV-A System Design Parameters\"\n",
        "  section_title = \"REPORT- SV-A System Design Parameters\"\n",
        "  start_indices = [m.start() for m in re.finditer(section_title, content)]\n",
        "\n",
        "  # Sections to exclude\n",
        "  exclude_titles = [\n",
        "      \"REPORT- SS-C\", \"REPORT- SS-D\", \"REPORT- SS-H\",\n",
        "      \"REPORT- SS-R\", \"REPORT- SS-K\", \"REPORT- SS-L\", \"REPORT- SS-I\"\n",
        "  ]\n",
        "\n",
        "  # Step 3: Extract sections while excluding unwanted ones\n",
        "  sections = []\n",
        "  for i, start_index in enumerate(start_indices):\n",
        "      # Find the next occurrence or the end of the content\n",
        "      end_index = start_indices[i + 1] if i + 1 < len(start_indices) else len(content)\n",
        "      section = content[start_index:end_index]\n",
        "\n",
        "      # Check if the section contains any of the excluded titles\n",
        "      if any(exclude_title in section for exclude_title in exclude_titles):\n",
        "          continue  # Skip sections with excluded titles\n",
        "\n",
        "      # Include only sections with \"REPORT- SV-A System Design Parameters\"\n",
        "      if section_title in section:\n",
        "          sections.append(section)\n",
        "\n",
        "  # Step 4: Write the filtered sections to a new file\n",
        "  with open(output_file_path, 'w', encoding='ISO-8859-1') as output_file:\n",
        "      for section in sections:\n",
        "          output_file.write(section)\n",
        "          output_file.write('\\n\\n')  # Add a separator between sections\n",
        "\n",
        "  print(f\"Filtered sections have been saved to {output_file_path}\")\n",
        "\n",
        "  with open(output_file_path, 'r', encoding='ISO-8859-1') as file:\n",
        "      content_filtered = file.read()\n",
        "\n",
        "\n",
        "\n",
        "#Gen AI\n",
        "client = OpenAI()\n",
        "class System_eQUEST(BaseModel):\n",
        "    system_name_s: str = Field(..., description=\"System name\")\n",
        "    system_type: str = Field(..., description=\"System type\")\n",
        "    altitude_factor: float = Field(..., description=\"Altitude factor\")\n",
        "    floor_area_sqft: float = Field(..., description=\"Floor area in square feet\")\n",
        "    max_people: int = Field(..., description=\"Maximum number of people\")\n",
        "    outside_air_ratio: float = Field(..., description=\"Outside air ratio\")\n",
        "    cooling_capacity_kbtu_hr: float = Field(..., description=\"Cooling capacity in KBTU/hr\")\n",
        "    sensible_heat_ratio: float = Field(..., description=\"Sensible heat ratio (SHR)\")\n",
        "    heating_capacity_kbtu_hr: float = Field(..., description=\"Heating capacity in KBTU/hr\")\n",
        "    cooling_eir_btu_btu: float = Field(..., description=\"Cooling energy efficiency ratio in BTU/BTU\")\n",
        "    heating_eir_btu_btu: float = Field(..., description=\"Heating energy efficiency ratio in BTU/BTU\")\n",
        "    heat_pump_supp_heat_kbtu_hr: float = Field(..., description=\"Heat pump supplementary heat in KBTU/hr\")\n",
        "\n",
        "\n",
        "class FanSystem(BaseModel):\n",
        "    system_name_f: str = Field(..., description=\"System name\")\n",
        "    fan_type: str = Field(..., description=\"Fan type (e.g., SUPPLY, RETURN)\")\n",
        "    capacity_cfm: float = Field(..., description=\"Capacity in cubic feet per minute (CFM)\")\n",
        "    diversity_factor_frac: float = Field(..., description=\"Diversity factor (fraction)\")\n",
        "    power_demand_kw: float = Field(..., description=\"Power demand in kilowatts (kW)\")\n",
        "    fan_delta_t_f: float = Field(..., description=\"Fan delta temperature in Fahrenheit (°F)\")\n",
        "    static_pressure_in_water: float = Field(..., description=\"Static pressure in inches of water (in-H2O)\")\n",
        "    total_eff_frac: float = Field(..., description=\"Total efficiency (fraction)\")\n",
        "    mech_eff_frac: float = Field(..., description=\"Mechanical efficiency (fraction)\")\n",
        "    fan_placement: str = Field(..., description=\"Fan placement (e.g., BLOW-THRU, DRAW-THRU)\")\n",
        "    fan_control: str = Field(..., description=\"Fan control method (e.g., BY USER, AUTOMATIC)\")\n",
        "    max_fan_ratio_frac: float = Field(..., description=\"Maximum fan ratio (fraction)\")\n",
        "    min_fan_ratio_frac: float = Field(..., description=\"Minimum fan ratio (fraction)\")\n",
        "\n",
        "\n",
        "from typing import List, Union\n",
        "\n",
        "#class SystemCollection(BaseModel):\n",
        "#    systems: List[System_eQUEST] = Field(..., description=\"List of systems\")\n",
        "\n",
        "class SystemCollection(BaseModel):\n",
        "    systems: List[Union[System_eQUEST, FanSystem]] = Field(..., description=\"List of systems (System_eQUEST and FanSystem)\")\n",
        "\n",
        "#class SystemCollection(BaseModel):\n",
        "#    systems: List[Union[System_eQUEST, FanSystem, ZoneSystem]] = Field(..., description=\"List of systems (System_eQUEST, FanSystem, and ZoneSystem)\")\n",
        "\n",
        "\n",
        "def structured():\n",
        "  global completion\n",
        "  completion = client.beta.chat.completions.parse(\n",
        "      model=\"gpt-4o-2024-08-06\",\n",
        "      temperature = 0,\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": \"You are an expert at structured data extraction. You will be given unstructured text from eQuest and should convert it into the given structure for all systems in the file.\"},\n",
        "          {\"role\": \"user\", \"content\": f\"{content_filtered}\"}\n",
        "      ],\n",
        "      response_format=SystemCollection,\n",
        "  )\n",
        "\n",
        "\n",
        "#excel_file_path = \"/content/df_system_equest.xlsx\"\n",
        "#df_system_equest.to_excel(excel_file_path, index=False)\n",
        "\n",
        "#excel_file_path = \"/content/df_fan_system.xlsx\"\n",
        "#df_fan_system.to_excel(excel_file_path, index=False)\n",
        "\n",
        "# Create a BytesIO buffer to write the Excel file in memory.\n",
        "buffer = io.BytesIO()\n",
        "\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"eQuest data extractor\")\n",
        "st.write(\"Powered by GPT-4o\")\n",
        "\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Load your .SIM file\")\n",
        "if uploaded_file is not None:\n",
        "    # Convert the uploaded file content to a string, assuming it uses ISO-8859-1 encoding\n",
        "    stringio = StringIO(uploaded_file.getvalue().decode('ISO-8859-1'))\n",
        "\n",
        "    # Step 1: Read the file content\n",
        "    content = stringio.read()\n",
        "\n",
        "    extraction()\n",
        "    structured()\n",
        "    system_response = completion.choices[0].message.parsed\n",
        "    response = system_response.systems\n",
        "\n",
        "    system_equest_list = [item for item in response if isinstance(item, System_eQUEST)]\n",
        "    fan_system_list = [item for item in response if isinstance(item, FanSystem)]\n",
        "    # Convert to DataFrames\n",
        "    df_system_equest = pd.DataFrame([item.dict() for item in system_equest_list])\n",
        "    df_fan_system = pd.DataFrame([item.dict() for item in fan_system_list])\n",
        "\n",
        "\n",
        "    # Create a Pandas Excel writer using XlsxWriter as the engine.\n",
        "    with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:\n",
        "      # Write each dataframe to a different worksheet.\n",
        "      df_system_equest.to_excel(writer, sheet_name='SystemEquest')\n",
        "      df_fan_system.to_excel(writer, sheet_name='FanSystem')\n",
        "\n",
        "    # Provide a download button for the user to download the file.\n",
        "    st.download_button(\n",
        "        label=\"Download Excel File\",\n",
        "        data=buffer.getvalue(),\n",
        "        file_name=\"system_fan_data.xlsx\",\n",
        "        mime=\"application/vnd.ms-excel\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZcWJ8FPJXPM",
        "outputId": "720ad649-2196-46bd-e929-d9a87476676a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    }
  ]
}